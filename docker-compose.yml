# version: '3'
services:

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    container_name: namenode
    volumes:
      - /tmp/hdfs/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop-hive.env
    ports:
      - "50080:50070"
    networks:
      net_pet:
        ipv4_address: 172.27.1.5

  datanode:
    build: ./datanode/
    container_name: datanode
    volumes:
      - /tmp/hdfs/datanode:/hadoop/dfs/data
      - ./bank:/bank
      - ./datanode:/datanode
    env_file:
      - ./hadoop-hive.env
    environment:
      SERVICE_PRECONDITION: "namenode:50070"
    depends_on:
      - namenode
    ports:
      - "50075:50075"
    networks:
      net_pet:
        ipv4_address: 172.27.1.6

  mysql:
    container_name: mysql-container
    build: ./mysql/
    restart: always
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=db_movies_netflix_transact
    ports:
      - '3310:3306'
    volumes:
      - ./mysqldata:/mysql/data
    env_file:
      - ./mysql/variables.env
    # networks:
    #   net_pet:
    #     ipv4_address: 172.27.1.2
  
  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    env_file:
      - ./hadoop-hive.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
    ports:
      - "10000:10000"
    depends_on:
      - hive-metastore
    networks:
      net_pet:
        ipv4_address: 172.27.1.7

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:50070 datanode:50075 hive-metastore-postgresql:5432"
    ports:
      - "9083:9083"
    depends_on:
      - hive-metastore-postgresql
    networks:
      net_pet:
        ipv4_address: 172.27.1.8

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql
    depends_on:
      - datanode
    networks:
      net_pet:
        ipv4_address: 172.27.1.9

  # spark-master:
  #   image: bde2020/spark-master:2.4.0-hadoop2.7
  #   container_name: spark-master
  #   ports:
  #     - 8080:8080
  #     - 7077:7077
  #   environment:
  #     - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
  #   env_file:
  #     - ./hadoop-hive.env
  #   networks:
  #     net_pet:
  #       ipv4_address: 172.27.1.10

  # spark-worker:
  #   image: bde2020/spark-worker:2.4.0-hadoop2.7
  #   container_name: spark-worker
  #   depends_on:
  #     - spark-master
  #   environment:
  #     - SPARK_MASTER=spark://spark-master:7077
  #     - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
  #     - HIVE_CORE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore/metastore
  #   ports:
  #     - 8081:8081
  #   env_file:
  #     - ./hadoop-hive.env
  #   networks:
  #     net_pet:
  #       ipv4_address: 172.27.1.11

  sqoop:
    build: ./sqoop
    container_name: sqoop
    depends_on:
      - mysql
      - hive-server
      - namenode
      - datanode
    tty: true
    stdin_open: true
    networks:
      net_pet:
        ipv4_address: 172.27.1.13

  zookeeper:
    image: bitnami/zookeeper:3.8
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    # networks:
    #   net_pet:
    #     ipv4_address: 172.27.1.14

  kafka:
    image: bitnami/kafka:3.6
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:29092,PLAINTEXT_HOST://0.0.0.0:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper
    # networks:
    #   net_pet:
    #     ipv4_address: 172.27.1.15

  # kafka-1:
  #   image: bitnami/kafka:3.6
  #   container_name: kafka-1
  #   ports:
  #     - "9092:9092"
  #   environment:
  #     - KAFKA_BROKER_ID=1
  #     - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
  #     - KAFKA_CFG_LISTENERS=PLAINTEXT://:29092,PLAINTEXT_HOST://0.0.0.0:9092
  #     - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka-1:29092,PLAINTEXT_HOST://localhost:9092
  #     - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
  #     - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
  #     - ALLOW_PLAINTEXT_LISTENER=yes
  #   depends_on:
  #     - zookeeper
  #   networks:
  #     net_pet:
  #       ipv4_address: 172.27.1.15

  # kafka-2:
  #   image: bitnami/kafka:3.6
  #   container_name: kafka-2
  #   environment:
  #     - KAFKA_BROKER_ID=2
  #     - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
  #     - KAFKA_CFG_LISTENERS=PLAINTEXT://:29092
  #     - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka-2:29092
  #     - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT
  #     - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
  #     - ALLOW_PLAINTEXT_LISTENER=yes
  #   depends_on:
  #     - zookeeper
  #   networks:
  #     net_pet:
  #       ipv4_address: 172.27.1.16

  # kafka-3:
  #   image: bitnami/kafka:3.6
  #   container_name: kafka-3
  #   environment:
  #     - KAFKA_BROKER_ID=3
  #     - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
  #     - KAFKA_CFG_LISTENERS=PLAINTEXT://:29092
  #     - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka-3:29092
  #     - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT
  #     - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
  #     - ALLOW_PLAINTEXT_LISTENER=yes
  #   depends_on:
  #     - zookeeper
  #   networks:
  #     net_pet:
  #       ipv4_address: 172.27.1.12

  debezium:
    image: quay.io/debezium/connect:3.0
    container_name: debezium
    depends_on:
      - kafka
      - mysql
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: debezium_config
      OFFSET_STORAGE_TOPIC: debezium_offset
      STATUS_STORAGE_TOPIC: debezium_status
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_REST_ADVERTISED_HOST_NAME: debezium
    ports:
      - "8083:8083"
    # networks:
    #   net_pet:
    #     ipv4_address: 172.27.1.12

  akhq:
    image: tchiotludo/akhq:latest
    # container_name: ui-kafka
    depends_on:
     - kafka
    ports:
      - "8080:8080"
    environment:
      TZ: America/Lima
      AKHQ_CONFIGURATION: | 
        akhq:
          connections:
            docker-kafka:
              properties:
                bootstrap.servers: "kafka:29092"
    # networks:
    #   net_pet:
    #     ipv4_address: 172.27.1.16

  airflow-postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - ./airflow/postgres-db:/var/lib/postgresql/data
    networks:
      net_pet:
        ipv4_address: 172.27.1.17

  airflow-webserver:
    image: apache/airflow:2.10.2
    restart: always
    depends_on:
      - airflow-postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "True"
      #SequentialExecutor: por defecto, ejecuta tareas de una en una (solo para pruebas).
      #CeleryExecutor: distribuido, usando workers (requiere Redis o RabbitMQ).
      
      #Número de ejecuciones paralelas:
      #AIRFLOW__CORE__PARALLELISM=32
      #AIRFLOW__CORE__DAG_CONCURRENCY=16
      #AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=2
      #parallelism: límite global de tareas simultáneas.
      #dag_concurrency: cuántas tareas de un DAG se pueden ejecutar a la vez.
      #max_active_runs_per_dag: cuántas instancias activas del mismo DAG se permiten.
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: your_secret_key_here
      AIRFLOW__CORE__FERNET_KEY: HX8v0oNrSo5anWMwA43Wmz7SpOfqYaagxkvOVfUuU0I=
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    ports:
      - "8082:8080"
    command: bash -c "airflow db migrate && airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com && airflow webserver"
    networks:
      net_pet:
        ipv4_address: 172.27.1.18

  airflow-scheduler:
    image: apache/airflow:2.10.2
    restart: always
    depends_on:
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: HX8v0oNrSo5anWMwA43Wmz7SpOfqYaagxkvOVfUuU0I=
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    command: airflow scheduler
    networks:
      net_pet:
        ipv4_address: 172.27.1.19

networks:
  net_pet:
    ipam:
      driver: default
      config:
        - subnet: 172.27.0.0/16