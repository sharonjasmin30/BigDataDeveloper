{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b26cbb28-096d-44de-b615-e88d6c122c8c",
   "metadata": {
    "id": "b26cbb28-096d-44de-b615-e88d6c122c8c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc86663-5c33-407f-8608-12bc730f159a",
   "metadata": {
    "id": "ebc86663-5c33-407f-8608-12bc730f159a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/14 02:30:17 WARN Utils: Your hostname, codespaces-b8cb10 resolves to a loopback address: 127.0.0.1; using 10.0.3.240 instead (on interface eth0)\n",
      "25/05/14 02:30:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/14 02:30:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/14 02:30:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/14 02:30:19 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9814d187-ba63-4c1c-8c58-5c543dc848ce",
   "metadata": {
    "id": "9814d187-ba63-4c1c-8c58-5c543dc848ce",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/14 02:30:21 WARN FileSystem: Failed to initialize fileystem hdfs://namenode:8020/user/datapath/datasets/customers: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode\n",
      "25/05/14 02:30:21 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: hdfs://namenode:8020/user/datapath/datasets/customers/.\n",
      "java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode\n",
      "\tat org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:466)\n",
      "\tat org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:134)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:374)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.initDFSClient(DistributedFileSystem.java:202)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:187)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:646)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.UnknownHostException: namenode\n",
      "\t... 31 more\n",
      "25/05/14 02:30:21 WARN FileSystem: Failed to initialize fileystem hdfs://namenode:8020/user/datapath/datasets/customers: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "java.net.UnknownHostException: namenode",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhdfs://namenode:8020/user/datapath/datasets/customers/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/sql/readwriter.py:615\u001b[39m, in \u001b[36mDataFrameReader.text\u001b[39m\u001b[34m(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001b[39m\n\u001b[32m    613\u001b[39m     paths = [paths]\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: java.net.UnknownHostException: namenode"
     ]
    }
   ],
   "source": [
    "spark.read.text(\"hdfs://namenode:8020/user/datapath/datasets/customers/\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6638358-c289-4c4a-bc9a-89d5d70c2b5f",
   "metadata": {
    "id": "b6638358-c289-4c4a-bc9a-89d5d70c2b5f"
   },
   "outputs": [],
   "source": [
    "CUSTOMERS_DATA =   'hdfs://namenode:8020/user/datapath/datasets/customers'\n",
    "DEPARTMENTS_DATA = 'hdfs://namenode:8020/user/datapath/datasets/departments'\n",
    "CATEGORIES_DATA =  'hdfs://namenode:8020/user/datapath/datasets/categories'\n",
    "PRODUCTS_DATA =    'hdfs://namenode:8020/user/datapath/datasets/products'\n",
    "ORDERS_DATA =      'hdfs://namenode:8020/user/datapath/datasets/orders'\n",
    "ORDER_ITEMS_DATA = 'hdfs://namenode:8020/user/datapath/datasets/order_items'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b70dc-790e-4308-81d0-334df810c022",
   "metadata": {
    "id": "656b70dc-790e-4308-81d0-334df810c022",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the schema, corresponding to a line in the csv data file for Customer\n",
    "customers_schema = StructType([\n",
    "    StructField('customer_id',       IntegerType(), nullable=True),\n",
    "    StructField('customer_fname',    StringType(), nullable=True),\n",
    "    StructField('customer_lname',    StringType(), nullable=True),\n",
    "    StructField('customer_email',    StringType(), nullable=True),\n",
    "    StructField('customer_password', StringType(), nullable=True),\n",
    "    StructField('customer_street',   StringType(), nullable=True),\n",
    "    StructField('customer_city',     StringType(), nullable=True),\n",
    "    StructField('customer_state',    StringType(), nullable=True),\n",
    "    StructField('customer_zipcode',  StringType(), nullable=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997703cd-7967-46d8-afe4-ec3305c694c0",
   "metadata": {
    "id": "997703cd-7967-46d8-afe4-ec3305c694c0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "departments_schema = StructType([\n",
    "    StructField('department_id',   IntegerType(), nullable=True),\n",
    "    StructField('department_name', StringType(), nullable=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe992f-683b-4291-8f3f-d8ed420add7f",
   "metadata": {
    "id": "0abe992f-683b-4291-8f3f-d8ed420add7f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories_schema = StructType([\n",
    "    StructField('category_id',            IntegerType(), nullable=True),\n",
    "    StructField('category_department_id', IntegerType(), nullable=True),\n",
    "    StructField('category_name',          StringType(), nullable=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34303581-f94a-4bdd-b4c9-26b9aef0c6de",
   "metadata": {
    "id": "34303581-f94a-4bdd-b4c9-26b9aef0c6de",
    "tags": []
   },
   "outputs": [],
   "source": [
    "products_schema = StructType([\n",
    "    StructField('product_id',          IntegerType(), nullable=True),\n",
    "    StructField('product_category_id', IntegerType(), nullable=True),\n",
    "    StructField('product_name',        StringType(), nullable=True),\n",
    "    StructField('product_description', StringType(), nullable=True),\n",
    "    StructField('product_price',       FloatType(), nullable=True),\n",
    "    StructField('product_image',       StringType(), nullable=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256ed06-5dbc-4cd8-a8c5-4b878fb115b9",
   "metadata": {
    "id": "1256ed06-5dbc-4cd8-a8c5-4b878fb115b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders_schema = StructType([\n",
    "    StructField('order_id',          IntegerType(), nullable=True),\n",
    "    StructField('order_date',        StringType(), nullable=True),\n",
    "    StructField('order_customer_id', IntegerType(), nullable=True),\n",
    "    StructField('order_status',      StringType(), nullable=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea932c22-9046-450a-9cc6-144d0d57bbe4",
   "metadata": {
    "id": "ea932c22-9046-450a-9cc6-144d0d57bbe4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "order_items_schema = StructType([\n",
    "    StructField('order_item_id',            IntegerType(), nullable=True),\n",
    "    StructField('order_item_order_id',      IntegerType(), nullable=True),\n",
    "    StructField('order_item_product_id',    IntegerType(), nullable=True),\n",
    "    StructField('order_item_quantity',      IntegerType(), nullable=True),\n",
    "    StructField('order_item_subtotal',      FloatType(), nullable=True),\n",
    "    StructField('order_item_product_price', FloatType(), nullable=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0025e9-fb42-4f4e-89a7-ae7488e0c5db",
   "metadata": {
    "id": "3f0025e9-fb42-4f4e-89a7-ae7488e0c5db"
   },
   "source": [
    "### Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9195dd-abb2-4d88-b81c-e82884f0a365",
   "metadata": {
    "id": "7e9195dd-abb2-4d88-b81c-e82884f0a365",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "customers_df = spark.read.csv(path=CUSTOMERS_DATA, schema=customers_schema)\n",
    "customers_df.cache()\n",
    "\n",
    "departments_df = spark.read.csv(path=DEPARTMENTS_DATA, schema=departments_schema)\n",
    "departments_df.cache()\n",
    "\n",
    "categories_df = spark.read.csv(path=CATEGORIES_DATA, schema=categories_schema)\n",
    "categories_df.cache()\n",
    "\n",
    "products_df = spark.read.csv(path=PRODUCTS_DATA, schema=products_schema)\n",
    "products_df.cache()\n",
    "\n",
    "orders_df = spark.read.csv(path=ORDERS_DATA, schema=orders_schema)\n",
    "orders_df.cache()\n",
    "\n",
    "order_items_df = spark.read.csv(path=ORDER_ITEMS_DATA, schema=order_items_schema)\n",
    "order_items_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1342588-7184-40b5-9a2f-0095a14a1631",
   "metadata": {
    "id": "d1342588-7184-40b5-9a2f-0095a14a1631",
    "tags": []
   },
   "outputs": [],
   "source": [
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910aed12-2e7d-4914-b4dc-6ee2f7154f6f",
   "metadata": {
    "id": "910aed12-2e7d-4914-b4dc-6ee2f7154f6f"
   },
   "source": [
    "### Creamos vistas temporales para trabajar con spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e628052-2e9a-4baa-b9e0-c78442b516df",
   "metadata": {
    "id": "0e628052-2e9a-4baa-b9e0-c78442b516df",
    "tags": []
   },
   "outputs": [],
   "source": [
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "customers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a033bbf-d99b-4256-bd14-4feff182975d",
   "metadata": {
    "id": "0a033bbf-d99b-4256-bd14-4feff182975d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "departments_df.createOrReplaceTempView(\"departments\")\n",
    "departments_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2143259b-5dfe-44e0-accd-9a24dd410335",
   "metadata": {
    "id": "2143259b-5dfe-44e0-accd-9a24dd410335",
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed0b88-5c30-4966-b974-e81db295774b",
   "metadata": {
    "id": "48ed0b88-5c30-4966-b974-e81db295774b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "order_items_df.createOrReplaceTempView(\"order_items\")\n",
    "order_items_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b62114-252d-4755-8a30-650187a442c9",
   "metadata": {
    "id": "c3b62114-252d-4755-8a30-650187a442c9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "products_df.createOrReplaceTempView(\"products\")\n",
    "products_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd1eae-d128-4e50-a6ad-9e7c5cac2cae",
   "metadata": {
    "id": "3dfd1eae-d128-4e50-a6ad-9e7c5cac2cae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories_df.createOrReplaceTempView(\"categories\")\n",
    "categories_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6c157-8b81-4eba-8b9a-d335cfab0ef3",
   "metadata": {
    "id": "f4a6c157-8b81-4eba-8b9a-d335cfab0ef3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
